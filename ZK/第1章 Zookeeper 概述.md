# 1.1 Zookeeper简介

​	***`ZooKeeper`*** 由雅虎研究院开发,后来捐赠给了 **`Apache`**。**`ZooKeeper`** 是一个开源的**分布式应用程序协调服务器**，其为分布式系统提供**一致性服务**。其一致性是通过==基于 `Paxos` 算法的`ZAB` 协议==完成的。其主要功能包括:**配置维护**、**域名服务**、**分布式同步**、**集群管理**等。	

## 1.1.1 功能简介

### （1）配置维护

![](https://ws1.sinaimg.cn/large/006tNc79gy1g03p5pxfzoj310c0mkwj7.jpg)

​	分布式系统中,很多服务都是部署在集群中的,即多台服务器中部署着完全相同的应用,起着完全相同的作用。当然,集群中的这些服务器的配置文件是完全相同的。

​	若集群中服务器的配置文件需要进行修改,那么我们就需要逐台修改这些服务器中的配置文件。如果我们集群服务器比较少,那么这些修改还不是太麻烦,但如果集群服务器特别多,比如某些大型互联网公司的 **Hadoop** 集群有数千台服务器,那么纯手工的更改这些配置文件几乎就是一件不可能完成的任务。即使使用大量人力进行修改可行,但过多的人员参与,出错的概率大大提升,对于集群的形成的危险是很大的。

​	这时候 **`Zookeeper`** 就可以派上用场了。其对于配置文件的维护采用的是“**发布/订阅模型**”。发布者将修改好的集群的配置文件发布到 `Zookeeper` 服务器的文件系统中,那么订阅者马上就可以接收到通知,并主动去同步`Zookeeper` 里的配置文件。`Zookeeper` 具有**同步操作的原子性**,确保每个集群服务器的配置文件都能被正确的更新。

### （2）域名服务

![](https://ws4.sinaimg.cn/large/006tNc79gy1g03pdmgjidj30zb0u0ti0.jpg)

​	在分布式应用中,一个项目包含多个工程,而这些工程中,有些工程是专门为其它工程提供服务的。一个项目中可能会存在多种提供不同服务的工程,而一种服务又可能存在多个提供者(服务器)。所以,用于消费这些服务的客户端工程若要消费这些服务,就变的异常的复杂了。

​	此时,**`Zookeeper`** 就可以上场了。<u>为每个服务起个名称,将这些**服务的名称**与提供这些服务的**主机地址**注册到 **`Zookeeper`** 中,形成一个**服务映射表**</u>。服务消费者只需要通过**服务名称**即可享受到服务,而无需了解服务具体的提供者是谁。服务的减少、添加、变更,只需修改 **`Zookeeper`** 中的服务映射表即可。

​	阿里的 Dubbo 就是使用 Zookeeper 作为服务域名服务器的。

### （3）分布式同步

![](https://ws3.sinaimg.cn/large/006tNc79gy1g03pk9l745j30xg0osdlm.jpg)

​	在分布式系统中,很多运算(对请求的处理)过程是由分布式机群中的若干服务器共同计算完成的,并且**它们之间的运算还具有逻辑上的先后顺序**。如何保证这些服务器运行期间的同步性呢?

​	使用 **`Zookeeper`** 可以协调这些服务器间运算的过程。让这些服务器都同时监听 **`Zookeeper`**上的同一个 **`znode`**(**`Zookeeper`** 文件系统中的一个数据存储节点),一旦其中一个服务器**`Update`** 了 **`znode`**,那么另一个相应服务器能够收到通知,并作出相应处理。原理简述，即

* 服务执行有序，一个**`service`**执行完，下一个**`service`**执行。
* **`Znode`**可以存储所监听的主机（各主机在**`Znode`**上注册一个watcher值为0，执行完则0变成1，然后触发下一个**`service`**执行）
* **`Znode`**和主机之间通过**`ping`**、**`pong`**心跳来感知主机状态。
* 感知主机异常，会尝试修复，修复不了，则会记录日志到消息中间件。

### （4）集群管理

​	集群管理中<u>最麻烦</u>的就是**节点故障管理**。**`Zookeeper`** 允许其所监听的集群主机向其注册**`watcher`** 监听,而 zk 会在其文件系统中为该主机创建一个 **`znode`**,用于保存监听该主机的运行状态。当集群主机发生故障时,**`Zookeeper`** 会马上感知到。

​	**`Zookeeper`** 不仅可以发现故障,也会对故障进行甄别,如果该故障可以修复,**`Zookeeper`**会自动将其修复,若不能修复则会告诉系统管理员错误的原因让管理员迅速定位问题。

## 1.1.2 一致性要求

​	什么是 zk 的一致性呢?其需要满足以下几点要求:

### （1）顺序一致性

​	从同一个客户端发起的 n 多个事务请求,最终将会严格**按照其发起顺序**被应用到**`ZooKeeper`** 中。

### （2）原子性

​	**所有事务请求的结果在集群中所有机器上的应用情况是一致的**。也就是说要么整个集群所有主机都成功应用了某一个事务,要么都没有成功,不会出现集群中部分主机应用某事务成功,而另外一部分失败的情况。即：**无论集群哪个机器执行请求结果都是一致的**。

### （3）单一视图

​	无论客户端连接的是哪个 **`ZooKeeper`** 服务器,其看到的**服务端数据模型都是一致的**。

### （4）可靠性

​	一旦服务端成功地应用了一个事务,并完成对客户端的响应,那么该事务所引起的服务端状态变更将会被一直保留下来,除非有另一个事务又对其进行了变更。 即：**一旦执行成功，结果是不会改变的**。

### （5）实时性

​	通常人们看到实时性的第一反应是,一旦一个事务被成功应用,那么客户端能够立即从服务端上读取到这个事务变更后的最新数据状态。这里需要注意的是,**`ZooKeeper`** 仅仅保证在一段较短的时间内,客户端最终一定能够从服务端上读取到最新的数据状态。

# 1.2 Zookeeper中的重要概念

## 1.2.1 Session

​	**`Session`** 是指**客户端会话**。**`ZooKeeper`** 对外的服务端口默认是 **`2181`**,客户端启动时,首先会与 zk 服务器建立一个 `TCP` 长连接,从第一次连接建立开始,客户端会话的生命周期也开始了。通过这个长连接,客户端能够通过**心跳检测**保持与服务器的有效会话,也能够向`ZooKeeper` 服务器发送请求并接受响应,同时还能通过该连接接收来自服务器的 `Watcher` 事件通知。

​	`Session` 的 `SessionTimeout` 值用来设置一个客户端会话的超时时间。当由于<u>服务器压力太大</u>、<u>网络故障</u>或是<u>客户端主动断开连接</u>等各种原因导致客户端连接断开时,只要在 `SessionTimeout` 规定的时间内客户端能够重新连接上集群中任意一台服务器,那么之前创建的会话仍然有效。

## 1.2.2 Znode

​	**`zookeeper`** 的文件系统采用**树形层次化的目录结构**,与 **`Unix`** 文件系统非常相似。每个目录在 **`zookeeper`** 中叫做一个 **`znode`**,每个 **`znode`** 拥有一个唯一的路径标识,即名称。**`Znode`**可以包含数据和子 **`znode`**(临时节点不能有子 **`znode`**)。**`Znode`** 中的数据可以有多个版本,所以查询某路径下的数据需带上版本号。客户端应用可以在 **`znode`** 上设置**监视器**(**`Watcher`**)。

## 1.2.3 Watcher机制

​	zk 提供了分布式数据的发布订阅功能,一个发布者能够让多个订阅者同时监听某一主题对象。当这个主题对象状态发生变化时,会通知所有订阅者,使它们能够做出相应的处理。 **zk 通过 `Watcher` 机制实现了发布订阅模式**。 

​	**zk 引入了 `watcher` 机制来实现这种分布式的通知功能**。首先客户端需要向 zk 注册一个 Watcher 监听,当 zk 中的指定事件触发了 zk 中的监听器 Watcher,那么 watcher 就会向指定客户端发送事件通知,而这个事件通知则是通过 **`TCP`** 长连接的 **`Session`** 完成的。 

## 1.2.4 ACL

​	**`ACL`** 全称为 `Access Control List`(访问控制列表),**用于控制资源的访问权限**,是 zk 数据安全的保障。**zk 利用 `ACL` 策略控制 `znode` 节点的访问权限**,如节点数据读写、节点创建、节点删除、读取子节点列表、设置节点权限等。 

​	在传统的文件系统中,**`ACL`** 分为两个维度:**组**与**权限**。一个属组可以包含多种权限,一个文件或目录拥有了某个组的权限即拥有了组里的所有权限。文件或子目录默认会继承其父目录的 **`ACL`**。 

​	而在 **`Zookeeper`** 中,**`znode`** 的 **`ACL`** 是**没有继承关系**的,每个 **`znode`** 的权限都是**独立控制**的,只有客户端满足 **`znode`** 设置的权限要求时,才能完成相应的操作。Zookeeper 的 ACL 分为三个维度:**授权策略** **`scheme`**、**用户 **id、**用户权限** **`permission`**。 

# 1.3 Paxos算法

## 1.3.1 算法简介

​	**`Paxos`** 算法是莱斯利·兰伯特(`Leslie Lamport`)1990 年提出的**一种基于消息传递的、具有高容错性的一致性算法**。`Google Chubby` 的作者 `Mike Burrows` 说过,世上只有一种一致性算法, 那就是 Paxos,所有其他一致性算法都是 Paxos 算法的不完整版。Paxos 算法是一种公认的晦涩难懂的算法,并且工程实现上也具有很大难度。较有名的 Paxos 工程实现有 `Google Chubby`、 `ZAB`、微信的 `PhxPaxos` 等。 

​	**`Paxos`** 算法是用于解决什么问题的呢?**`Paxos`** 算法要解决的问题是：**在分布式系统中如何就某个决议达成一致**。 

## 1.3.2 Paxos与拜占庭将军问题

​	拜占庭将军问题是由 `Paxos` 算法作者莱斯利·兰伯特提出的**点对点通信中的基本问题**。该问题要说明的含义是：<u>在存在**消息丢失的不可靠信道**上试图**通过消息传递**的方式达到**一致性**是不可能的</u>。Paxos 算法的前提是不存在拜占庭将军问题,即信道是**安全**的、**可靠**的,集群节点间传递的**消息是不会被篡改**的。 

​	一般情况下,分布式系统中各个节点间采用两种通讯模型:**共享内存**(`Shared Memory`)、 **消息传递**(`Messages Passing`)。而 **`Paxos`** 是基于**消息传递**

![](https://ws4.sinaimg.cn/large/006tNc79gy1g04koxdkxoj314a0lwn6x.jpg)

## 1.3.3 算法描述

### （1）三种角色

​	在 **`Paxos`** 算法中有三种角色分别具有三种不同的行为。但很多时候,一个进程可能同时充当着多种角色。 

* **`Proposer`**：提案(`Proposal`)的**提议者**。 

* **`Acceptor`**：提案的**表决者**,即是否 accept 该提案。只有半数以上的 Acceptor 接受了某提案,那么该提案者被认定为“选定”。 

* **`Learners`**：提案的**学习者**。当提案被选定时,其要执行提案内容。 

  ​	

  ​	一个提案的表决者(`Acceptor`)会存在多个,但在一个集群中,提议者(`Proposer`)也是可能存在多个的,不同的提议者(`Proposer`)会提出不同的提案。一致性算法则可以保证如下几点: 

* 没有提案被提出则不会有提案被选定。 

* 每个提议者在提出提案时都会首先获取到一个具有**全局唯一性**的、**递增**的**提案编号N**, 即在整个集群中是唯一的编号N,然后将该编号赋予其要提出的提案。 

* 每个表决者在 `accept` 某提案后,会将该提案的编号 N 记录在本地,这样每个表决者中保存的已经被 accept 的提案中会存在一个编号最大的提案,其编号假设为 `maxN`。每个表决者仅会 `accept` 编号大于自己本地 `maxN` 的提案。 

* 在众多提案中最终只能有一个提案被选定。 
* 一旦一个提案被选定,则其它服务器会主动同步(Learn)该提案到本地。 

### （2）算法过程描述

​	Paxos 算法的执行过程划分为两个阶段：**准备阶段** **`prepare`** 与**接受阶段** **`accept`**。

![](https://ws3.sinaimg.cn/large/006tNc79gy1g04l98g8zuj30zc0l4wpu.jpg) 

下面过程，以上图为模型进行分析

#### A、prepare阶段

1. 提议者(`Proposer`)准备提交一个编号为 `N` 的提议,于是其首先向所有表决者(`Acceptor`)发送 prepare(N)请求,用于试探集群是否支持该编号的提议。 
2. 每个表决者(`Acceptor`)中都保存着自己曾经 `accept` 过的提议中的最大编号 `maxN`。当一个表决者接收到其它主机发送来的 `prepare(N)`请求时,其会比较 `N` 与 maxN 的值。有以下几种情况: 
   1. 若 `N` 小于 `maxN`,则说明该提议已过时,当前表决者采取不回应或回应 `Error` 的方式来拒绝该 `prepare` 请求; 
   2. 若 `N` 大于 `maxN`,则说明该提议是可以接受的,当前表决者会首先将该 N 记录下来, 并将其曾经已经 accept 的编号最大的提案 `Proposal(myid,maxN,value)`反馈给提议者, 以向提议者展示自己支持的提案意愿。其中第一个参数 `myid` 表示表决者 `Acceptor` 的标识 `id`；第二个参数表示其曾接受的提案的最大编号 `maxN`；第三个参数表示该提案的真正内容 `value`。当然,若当前表决者还未曾 `accept` 过任何提议,则会将 `Proposal(myid,null,null)`反馈给提议者。 
   3. 在 `prepare` 阶段 `N` 不可能等于 `maxN`。这是由 `N` 的生成机制决定的。要获得 `N` 的值, 其必定会**在原来数值的基础上采用同步锁方式增一**。

#### B、accept阶段

1. 当提议者(`Proposer`)发出 `prepare(N)`后,若收到了超过半数的表决者(`Accepter`)的反馈, 那么该提议者就会将其真正的提案 `Proposal(N,value)`发送给所有的表决者。 

2. 当表决者(`Acceptor`)接收到提议者发送的 `Proposal(N,value)`提案后,会再次拿出自己曾经 accept 过的提议中的最大编号 `maxN`,及曾经记录下的 `prepare` 的最大编号,让 `N` 与它们进行比较,若 `N` 大等于于这两个编号,则当前表决者 `accept` 该提案,并反馈给提议者。若 `N` 小于这两个编号,则表决者采取不回应或回应 `Error` 的方式来拒绝该提议。 

3. 若提议者没有接收到超过半数的表决者的 `accept` 反馈,则重新进入 `prepare` 阶段,递增提案号,重新提出 `prepare` 请求。若提议者接收到的反馈数量超过了半数,则其会向外广播两类信息: 

   1. 向曾 `accept` 其提案的表决者发送“**可执行数据同步信号**”,即让它们执行其曾接收 

      到的提案; 

   2. 向未曾向其发送 accept 反馈的表决者发送**“提案 + 可执行数据同步信号**”,即让它们接受到该提案后马上执行。

### （3）算法过程举例

![](https://ws1.sinaimg.cn/large/006tNc79gy1g04m01esmaj30sc0hoafv.jpg)

​	假设有三台主机,它们要从中选出一个 `Leader`。这三台主机在不同的时间分别充当着提案的提议者`Proposer`、表决者 `Acceptor` 及学习者 `Learner` 三种不同的角色。 

​	这里我们来模拟一个场景:每个提议者(`Proposer`)都想提议自己要当集群的 `Leader`,假设三个提议者**Proposer-1**、**Proposer-2**、**Proposer-3** 提议的消息初始编号依次为 2、1、3。说 明 **Proposal-2** 是第一个产生提案者,其次是 **Proposal-1** 与 **Proposal-3**。每个提议者都要将消息发送给所有的表决者(`Acceptor`),为了便于理解,假设都只有两个(超过半数)表决者收到消息:**Accepter-2** 与 **Acceptor-3** 收到了 **Proposer-2** 的消息**;Accepter-1** 与 **Acceptor-2** 收到了 **Proposer-1** 的消息;**Accepter-2** 与 **Acceptor-3** 收到了 **Proposer-3** 的消息。 

#### A、prepare阶段

​	假设 **Proposer-1** 发送的 `prepare(2)`消息先到达 **Acceptor-1** 和 **Acceptor-2**。因为它们之前都没有接收过 `prepare` 请求,所以它们都是直接接受该请求,并将 `Proposal(server1-Id,null,null)` 与 `Proposal(server2-Id,null,null)`反馈给提议者 **Proposer-1**,同时 **Acceptor-1** 和 **Acceptor-2** 记录下其目前收到的最大提议编号 `maxN` 为 2,即其以后不会再接受编号小于 2 的请求。**Proposer-1** 收到了超过半数的反馈。 

​	紧接着 **Proposer-2** 的 `prepare(1)`消息到达 **Acceptor-2** 和 **Acceptor-3**。由于 Acceptor-3 还未曾接受过其它请求,所以其直接接受 **Proposer-2** 的 `prepare(1)`的请求,并返回 **Proposer-2** `Proposal(server3-Id,null,null)`。**Acceptor-3** 会记录下其目前收到的最大提议编号 `maxN` 为 1, 即其以后不会再接受编号小于 1 的请求。但是,对于 **Acceptor-2** 来说,由于其已经接受 **Proposer-1** 的 `prepare(2)`的请求,所以其不再接收编号小于 2 的请求,所以 **Acceptor-2** 拒绝 **Proposer-2** 的 `parepare(1)`请求。**Proposer-2** 仅收到一个反馈,未超过半数的反馈。 

​	最后 **Proposer-3** 的 `prepare(3)`请求消息到达 **Acceptor-2** 和 **Acceptor-3**,它们都接受过请求。 但编号 3 的消息大于 **Acceptor-2** 已接受的 2 编号消息,同时也大于 **Acceptor-3** 已接受的 1 编号消息,所以他们都接受了该 `prepare(3)`请求。由于 **Acceptor-2** 和 **Acceptor-3** 虽都接受过 prepare 请求,但却都未接受过真正的提案,所以它们在接受 `prepare(3)`请求时反馈给 **Proposer-3** 的仍是 `Proposal(server2-Id,null,null)`与 `Proposal(server3-Id,null,null)`。**Proposer-3** 收到了超过半数的反馈。 

​	此时由于 **Proposer-2** 没有收到过半的回复,所以其为提案重新指定新的编号 4,并发送 `prepare(4)`给 **Acceptor-2** 和 **Acceptor-3**,此时编号 4 大于它们已接受的提案编号 3,所以接受该请求,并将 `Proposal(server2-Id,null,null)`与 `Proposal(server3-Id,null,null)`返回 **Proposer-2**。 **Proposer-2** 收到了超过半数的反馈。 

​	注意,并不是所有的 `Proposer` 都必须要达到过半了才能进入第二阶段,这里只是一种情况而已。 

​	截止到目前,**Acceptor-1**、 **Acceptor-2** 和 **Acceptor-3** 接收到的 `prepare` 请求的最大的值分别为 2、4、4。 

#### B、accept阶段

##### a、Proposer的提交

* **Proposer-1** 收到过半反馈,但这些反馈的 `value` 为 `null`,所以 **Proposer-1** 就直接向 **Acceptor-1** 和 **Acceptor-2** 提交了 `Proposal(server1-Id,2,server1)`的提案。 
*  **Proposer-**2 收到过半反馈,但这些反馈的 `value` 为 `null`,所以 **Proposer-2** 就直接向 **Acceptor-2** 和 **Acceptor-3** 提交了 `Proposal(server2-Id,4,server2)`的提案。 
* **Proposer-**3 收到过半反馈,但这些反馈的 `value` 为 `null`,所以 **Proposer-3** 就直接向 **Acceptor-2** 和 **Acceptor-3** 提交了 `Proposal(server3-Id,3,server3)`的提案。 

##### b、Acceptor的表决

* **Acceptor-1** 和 **Acceptor-2** 接收到 **Proposer-1** 的提案 `Proposal(server1-Id,2,server1)`请求。 由于 **Acceptor-1** 不能接收编号小于 2 的提案,但可以接收等于 2 的提案,所以其通过了 该提案。但由于 **Acceptor-2** 不能接收编号小于 4 的提案,所以其拒绝了该提案。 
* ** Acceptor-**2 和 **Acceptor-3** 接收到 **Proposer-2** 的提案 `Proposal(server2-Id,4,server2)`,它们均通过了该提案。 
* ** Acceptor-**2 和 **Acceptor-3** 接收到 **Proposer-3** 的提案 `Proposal(server3-Id,3,server3)`,它们均拒绝了该提案。 

​	由于 **Acceptor-2** 和 **Acceptor-3** 已经通过了提案 `Proposal(server2-Id,4,server2)`,并达成了超过半数的一致性,`server2` 马上成为了 `Leader`,选举状态结束。`server2` 会发布广播给所有`Learner`,通知它们来同步数据。同步完成后,集群进入正常服务状态。

## 1.3.4 Paxos算法的活锁问题

### A、死锁

![](https://ws3.sinaimg.cn/large/006tNc79gy1g04n17a9icj30vm0eewi3.jpg)

​	线程 A 占用着资源 a,但其由于等待资源 b 而阻塞;同时,线程 B 占用着资源 b,但其由于等待资源 a 而阻塞。**双方均不释放自己所执有的资源而陷入的一种“锁死状态”称为死锁**。死锁需要具备以下几个条件:

* **互斥条件**:一个资源每次只能被一个进程使用 
* **请求与保持条件**:一个进程因请求资源而阻塞时,对已获得的资源保持不放 
* **不剥夺条件**:进程已获得的资源,在末使用完之前,不能强行剥夺
* **循环等待条件**:若干进程之间形成一种头尾相接的循环等待资源关系 

### B、活锁

​	活锁指的是执行者没有被阻塞,由于某些条件没有满足,导致一直重复“**尝试—失败—尝试—失败**”的过程。处于活锁的实体是在不断的改变状态,活锁有可能自行解开。例如,某进程对于某“竞争资源”的申请一直没有提到满足,于是就一直在不停的申请中。此时该进程所处的状态就称为“活锁”。

### C、两锁的区别

​	活锁和死锁的区别在于：处于活锁的实体并没有被阻塞,而是在不断的改变状态;而处于死锁的实体则表现为等待、阻塞;活锁有可能自行解开,死锁则不能。 

​	从操作系统理论角度来说,活锁状态的线程能够获取到“**时间片**“,拥有获取 CPU 资源的权限,即其一直处于”**执行态-就绪态**“的转换过程中,而非处于”**阻塞态**“。而死锁状态的进程则处于“**阻塞态**”,不具有获取 CPU 资源的权限,不能获得到”**时间片**“。 

> (进程的三种基本状态:执行态、就绪态、阻塞态)



​	前面所述的 `Paxos` 算法在实际工程应用过程中,根据不同的实际需求存在诸多不便之处,所以也就出现了很多对于基本 Paxos 算法的优化算法,以对 Paxos 算法进行改进,例如,**Multi Paxos、Fast Paxos、EPaxos**。

​	例如：<u>Paxos 算法存在“**活锁问题**”</u>,**`Fast Paxos`** 算法对 `Paxos` 算法进行了改进:其**只允许一个进程处理写请求,解决了活锁问题**。

​	为什么 `paxos` 算法中存在活锁问题呢?paxos 算法中每个进程均可提交提案,但必须要获取到一个全局的唯一编号 `N`,将该 `N` 值赋予提案。为了保证 `N` 的**唯一性**,对该 `N` 值的操作就必须要放到**同步锁(排它锁)**中。即,`N` 值就成为了“**竞争资源**”。若一个进程为了提交提案,一直在不停的申请资源 N,但每一次都没有分配给它。此时的该进程就处于“活锁” 状态。 

​	**Fast Paxos** 算法<u>只允许一个进程提交提案</u>,即其具有对 N 的唯一操作权。该方式解决了 “活锁”问题。 

# 1.4 ZAB协议

## 1.4.1 ZAB协议简介

​	**`ZAB`** 协议是 `Fast Paxos` 算法的一种工业实现算法。 

​	**`ZAB`** ,`Zookeeper Atomic Broadcast`,**zk 原子消息广播协议**,是专为 `ZooKeeper` 设计的**一种支持崩溃恢复的原子广播协议**,在 `Zookeeper` 中,<u>主要依赖 **`ZAB`** 协议来实现分布式数据一致性</u>。 

​	`Zookeeper` 使用一个单一主进程来接收并处理客户端的所有事务请求,即写请求。当服务器数据的状态发生变更后,集群采用 **`ZAB`** 原子广播协议,以事务提案 `Proposal` 的形式广播到所有的副本进程上。**`ZAB`** 协议能够保证一个全局的变更序列,即可以为每一个事务分配一个全局的递增编号 `xid`。 

​	当`Zookeeper` 客户端连接到 `Zookeeper` 集群的一个节点后,若客户端提交的是读请求, 那么当前节点就直接根据自己保存的数据对其进行响应;如果是写请求且当前节点不是 Leader,那么节点就会将该写请求转发给`Leader`,`Leader` 会以提案的方式广播该写操作,只要有超过半数节点同意该写操作,则该写操作请求就会被提交。然后 `Leader` 会再次广播给所有订阅者,即 `Learner`,通知它们同步数据。

![](https://ws2.sinaimg.cn/large/006tNc79gy1g04rihsk9dj315e0mq48o.jpg)

## 1.4.2 三类角色

为了避免 `Zookeeper` 的**单点问题**,zk 也是以**集群的形式**出现的。zk 集群中的角色主要有以下三类: 

* **`Leader`**:**zk 集群写请求的唯一处理者,并负责进行投票的发起和决议,更新系统状态**。 Leader 是很民主的,并不是说其在接收到写请求后马上就修改其中保存的数据,而是首先根据写请求提出一个提议,在大多数 `zkServer` 均同意时才会做出修改。 

* **`Follower`**:接收客户端请求,处理读请求,并向客户端返回结果;将写请求转给 `Leader`; 在选主(选 `Leader`)过程中参与投票。 

* **`Observer`**:可以理解为无选主投票权与写操作投票权的 `Flollower`,其不属于法定人数范围,主要是为了协助 `Follower` 处理更多的读请求。 

  

  这三类角色在不同的情况下又有一些不同的叫法。

* `Learner`：**学习者**,要从`Leader`中同步数据的`Server`。在集群处于正常服务状态时,`Follower` 与 `Observer` 相对于 `Leader` 来说,都是 `Learner`。 

* `QuorumServer`：**法定服务器**,具有投票权的主机。在 `Leader` 选举过程中,各个参与选举与被选举的主机都称为 `QuorumServer`。`QuorumServer` 中是不包含 `Observer` 的,在选举结束后这些 `QuorumServer` 中会产生一个 `Leader`,其它的就成为了 `Follower` 了。 

## 1.4.3 三种模式

​	ZAB 协议中对 zkServer 的状态描述有三种模式:**恢复模式**、**同步模式**和**广播模式**。这三种模式并没有十分明显的界线,它们相互交织在一起。

* **恢复模式**：在服务重启过程中,或在 Leader 崩溃后,就进入了恢复模式,恢复模式主要包含两个阶段：**Leader 的选举阶段**与**初始化同步阶段**。这两个阶段完成后 zk 集群恢复到了正常服务状态。
* **广播模式**：广播模式分为两类：**初始化广播**与**更新广播**。当 Leader 被选举出来后,Leader需要将自己拥有但其它 Server 可能没有的事务及自己的 epoch 广播给所有 Learner,这是初始化广播。在集群正常服务时,当 Leader 的事务提案被大多数 Follower 同意后, Leader 会修改自身数据,然后会将修改后的数据广播给其它 Learner,这是更新广播。 
* **同步模式**：与广播模式相对应,同步模式也分为两类：**初始化同步**及**更新同步**。当 Leader 选举出来并发布过其初始化广播后,所有 Learner 需要将 Leader 广播的事务及 epoch 同步到本地,这称为初始化同步。在集群正常服务时,当 Leader 发布了更新广播后,Learner 需要将 Leader 广播的事务同步到本地,这称为更新同步。 

## 1.4.4 三个数据

​	在 Zookeeper 中有三个很重要的数据：**`zxid`**、**`epoch`** 与 **`xid`**。 

* `zxid` 为 64 位长度的 Long 类型,其中高 32 位表示**纪元epoch**,低 32 位表示**事务标识 xid**。 即 zxid 由两部分构成：**`epoch`** 与 **`xid`**。 

* 每个 Leader 都会具有一个不同的 `epoch` 值,**表示一个时期、时代**。每一次新的选举结束后都会生成一个新的 `epoch`,新的 Leader 产生,则会更新所有 zkServer 的 `zxid` 中的 `epoch`。

* `xid` 则为 zk 的事务 `id`,每一个写操作都是一个事务,都会有一个 `xid`。`xid` 为一个依次递增的流水号。每一个写操作都需要由 Leader 发起一个提案,由所有 Follower 表决是否同意本次写操作,而每个提案都具有一个`zxid`。 

## 1.4.5 同步模式与广播模式

### （1）初始化同步

![](https://ws2.sinaimg.cn/large/006tKfTcly1g0c5qrgaflj30qh0jqq9j.jpg)

​	当完成 Leader 选举后,此时的 Leader 是准 Leader,其要进入到恢复模式下的初始化同步阶段,使准 Leader 转变为真正的 Leader。具体过程如下: 

1. 为了保证 Leader 向 Learner 发送提案的有序,Leader 会为每一个 Learner 服务器准备一个队列 
2. Leader 将那些没有被各个 Learner 服务器同步的事务封装为 Proposal 
3. Leader 将这些 Proposal 逐条发给各个 Learner 服务器,并在每一个 Proposal 后都紧跟一个 COMMIT 消息,表示该事务已经被提交,Learner 可以直接接收并执行 
4. Learner 接收来自于 Leader 的 Proposal,并将其更新到本地 
5. 当 Follower 更新成功后,会向准 Leader 发送 ACK 信息 (**Learner-3没有发送ACK信息，其实它是个observer，observer根本不需要反馈，因为Leader根本不用征求observer的意见，也就是说observer本身是没有反馈这个功能的**)
6. Leader 服务器在收到该来自 Follower 的 ACK 后就会将该 Follower 加入到真正可用的 Follower 列表 

### （2）消息广播算法

​	当集群中已经有过半的 Follower 完成了初始化状态同步,那么整个 zk 集群就进入到了 正常工作模式了。 

​	如果集群中的其他节点收到客户端的事务请求,那么这些 Learner 会将请求转发给 Leader 服务器。然后再执行如下的具体过程: 

![](https://ws2.sinaimg.cn/large/006tKfTcly1g0c5ytjtx2j30rt0i80zp.jpg)

1. Leader 接收到事务请求后,为事务赋予一个全局唯一的 64 位自增 id,即 zxid,通过 zxid 的大小比较即可实现事务的有序性管理,然后将事务封装为一个 Proposal。 
2. Leader根据Follower列表获取到所有Follower,然后再将Proposal通过这些Follower的队列将提案发送给各个 Follower。 
3. 当 Follower 接收到提案后,会先将提案的 zxid 与本地记录的事务日志中的最大的 zxid 进行比较。若当前提案的 zxid 大于最大 zxid,则将当前提案记录到本地事务日志中,并 向 Leader 返回一个 ACK。 
4. 当 Leader 接收到过半的 ACKs 后,Leader 就会向所有 Follower 的队列发送 COMMIT 消息,向所有 Observer 的队列发送 Proposal。 
5. 当 Follower 收到 COMMIT 消息后,就会将日志中的事务正式更新到本地。当 Observer 收到 Proposal 后,会直接将事务更新到本地。 

## 1.4.6 恢复模式的两个原则

​	当集群正在启动过程中,或 Leader 与超过半数的主机断连后,集群就进入了恢复模式。 对于要恢复的数据状态需要遵循两个原则：

### （1）已被处理过的消息不能丢

​	当 **Leader** 收到超过半数 **Follower** 的 **ACKs** 后,就向各个 **Follower** 广播 **COMMIT** 消息, 批准各个 Server 执行该写操作事务。当各个 **Server** 在接收到 **Leader** 的 **COMMIT** 消息后就会在本地执行该写操作,然后会向客户端响应写操作成功。 

​	但是如果在非全部 **Follower** 收到 **COMMIT** 消息之前 **Leader** 就挂了,这将导致一种后果:部分 **Server** 已经执行了该事务,而部分 **Server** 尚未收到 **COMMIT** 消息,所以其并没有执行该事务。当新的 **Leader** 被选举出,集群经过恢复模式后需要保证所有 **Server** 上都执行了那些已经被部分 **Server** 执行过的事务。 

### （2）被丢弃的消息不能再现

​	当 **Leader** 接收到事务请求并生成了 **Proposal**,但还未向 **Follower** 发送时就挂了。由于其他 **Follower** 并没有收到此 **Proposal**,即并不知道该 **Proposal** 的存在,因此在经过恢复模式重新选举产生了新的 **Leader** 后,这个事务被跳过。在整个集群尚未进入正常服务状态时, 之前挂了的 **Leader** 主机重新启动并注册成为了 **Follower**。但<u>由于保留了被跳过的 **Proposal**, 所以其与整个系统的状态是不一致的,需要将该 **Proposal** 删除。</u> 

​	那么如何能够将其删除呢?根据 **epoch** 的生成规则,新 **Leader** 的 **epoch** 值一定大于当前集群中任何主机记录的事务的 `zxid` 值(确切地说,是大于 `zxid` 的高 32 位),因为 **epoch** 值是集群中最大 zxid 的高 32 位值加一后的值。当新的 **Leader** 产生后,其会将它的 **epoch** 广 播给所有 **Learner**,并且会将所有 **Learner** 中所有旧的未被 **COMMIT** 过的 **Proposal** 清除。 

## 1.4.7 Leader选举

### （1）Leader选举中的基本概念

#### A、myid

​	这是 **zk** 集群中服务器的唯一标识,称为 **myid**。例如,有三个 **zk** 服务器,那么编号分别是 1,2,3。

#### B、逻辑时钟

​	逻辑时钟,**Logicalclock**,是一个整型数,该概念在选举时称为 **logicalclock**,而在选举结束后的 `zxid` 中则为 `epoch` 的值。即 `epoch` 与 **logicalclock** 是同一个值,在不同情况下的不同名称。

#### C、zk状态

zk 集群中的每一台主机,在不同的阶段会处于不同的状态。每一台主机具有**四种状态**。 

* **LOOKING**：选举状态(查找 **Leader** 的状态)。 
* **FOLLOWING**：跟随状态,同步 **Leader** 状态。处于该状态的服务器称为 **Follower**。 
* **OBSERVING**：观察状态,同步 Leader 状态。处于该状态的服务器称为 **Observer**。 
* **LEADING**：领导状态。处于该状态的服务器称为 **Leader**。 

### （2）Leader选举算法

​	在集<u>群启动过程中的 **Leader** 选举过程(算法)</u>与 **Leader** <u>断连后的 **Leader** 选举过程</u>**稍微有一些区别**,基本相同。 

#### A、集群启动中的Leader选举

![](https://ws3.sinaimg.cn/large/006tKfTcly1g0egja46icj31690lxk79.jpg)

​	若进行 Leader 选举,则至少需要两台主机,这里以三台主机组成的集群为例。 

​	在集群初始化阶段,当第一台服务器 **Server1** 启动时,其会给自己投票,然后发布自己的投票结果。投票包含所推举的服务器的 `myid` 和 `ZXID`,使用`(myid, ZXID)`来表示,此时 **Server1** 的投票为`(1, 0)`。由于其它机器还没有启动所以它收不到反馈信息,**Server1** 的状态一直属于`Looking`,即属于非服务状态。

​	当第二台服务器 **Server2** 启动时,此时两台机器可以相互通信,每台机器都试图找到**Leader**,选举过程如下:

1. 每个 **Server** 发出一个投票。此时 **Server1** 的投票为`(1, 0)`,**Server2** 的投票为`(2, 0)`,然后各自将这个投票发给集群中其他机器。
2. 接受来自各个服务器的投票。集群的每个服务器收到投票后,首先判断该投票的有效性,如检查是否是本轮投票、是否来自 LOOKING 状态的服务器。
3. 处理投票。针对每一个投票,服务器都需要将别人的投票和自己的投票进行 PK,PK规则如下:
   * 优先检查 `ZXID`。`ZXID` 比较大的服务器优先作为 **Leader**。
   * 如果 `ZXID` 相同,那么就比较 `myid`。`myid` 较大的服务器作为 Leader 服务器。
     对于 **Server1** 而言,它的投票是`(1, 0)`,接收 **Server2** 的投票为`(2, 0)`。其首先会比较两者的 `ZXID`,均为 **0**,再比较 `myid`,此时 **Server2** 的 `myid` 最大,于是 **Server1** 更新自己的投票为 `(2, 0),`然后重新投票。对于 Server2 而言,其无须更新自己的投票,只是再次向集群中所有主机发出上一次投票信息即可。 
4. 统计投票。每次投票后,服务器都会统计投票信息,判断是否已经有过半机器接受到相同的投票信息。对于 **Server1**、**Server2** 而言,都统计出集群中已经有两台主机接受了`(2, 0)` 的投票信息,此时便认为已经选出了新的 **Leader**,即 **Server2**。 
5. 改变服务器状态。一旦确定了 **Leader**,每个服务器就会更新自己的状态,如果是 **Follower**,那么就变更为 **FOLLOWING**,如果是 **Leader**,就变更为 **LEADING**。 
6. 添加主机。在新的 **Leader** 选举出来后 **Server3** 启动,其想发出新一轮的选举。但由于当前集群中各个主机的状态并不是 **LOOKING**,而是各司其职的正常服务,所以其只能是以 **Follower** 的身份加入到集群中。 

>* ZK集群只启动一台时，该服务一直处LOOKING状态，非服务状态，是不能用的；
>* leader选举中，最开始每个机器启动都是我选我，毛遂自荐； 

#### B、断连后的Leader选举

![](https://ws4.sinaimg.cn/large/006tKfTcly1g0egih9pn8j317a0sotqb.jpg)

​	在 **Zookeeper** 运行期间,**Leader** 与非 **Leader** 服务器各司其职,即便当有非 **Leader** 服务器宕机或新加入时也不会影响 **Leader**。但是若 **Leader** 服务器挂了,那么整个集群将暂停对外服务,进入新一轮的 **Leader** 选举,其过程和启动时期的 **Leader** 选举过程基本一致。 

​	假设正在运行的有 **Server1**、**Server2**、**Server3** 三台服务器,当前 **Leader** 是 **Server2**,若 某一时刻 **Server2** 挂了,此时便开始新一轮的 **Leader** 选举了。选举过程如下: 

1. 变更状态。**Leader** 挂后,余下的非 **Observer** 服务器都会将自己的服务器状态由 **FOLLOWING** 变更为**LOOKING**,然后开始进入 **Leader** 选举过程。 
2. 每个 **Server** 会发出一个投票,仍然会首先投自己。不过,在运行期间每个服务器上 的 `ZXID` 可能是不同,此时假定 **Server1** 的 `ZXID` 为 111,**Server3** 的 `ZXID` 为 333;在第一轮投票中,**Server1** 和 **Server3** 都会投自己,产生投票`(1, 111)`,`(3, 333)`,然后各自将投票发送给集群中所有机器。 
3. 接收来自各个服务器的投票。与启动时过程相同。集群的每个服务器收到投票后, 首先判断该投票的有效性,如检查是否是本轮投票、是否来自 **LOOKING** 状态的服务器。 
4. 处理投票。与启动时过程相同。针对每一个投票,服务器都需要将别人的投票和自己的投票进行 PK。对于 **Server1** 而言,它的投票是`(1, 111)`,接收 **Server3** 的投票为`(3, 333)`。 其首先会比较两者的 `ZXID`,Server3 投票的 `zxid` 为 333 大于 **Server1** 投票的 `zxid` 的 111,于是 **Server1** 更新自己的投票为`(3, 333)`,然后重新投票。对于 **Server3** 而言,其无须更新自己的投票,只是再次向集群中所有主机发出上一次投票信息即可。 
5. 统计投票。与启动时过程相同。对于 **Server1**、**Server2** 而言,都统计出集群中已经有两台主机接受了`(3, 333)`的投票信息,此时便认为已经选出了新的 **Leader**,即 **Server3**。 
6. 改变服务器的状态。与启动时过程相同。一旦确定了 **Leader**,每个服务器就会更新自己的状态。**Server1** 变更为 **FOLLOWING**,**Server3** 变更为 **LEADING**。 

# 1.5 CAP原则

## 1.5.1 简介

​	**CAP** 原则又称 **CAP** 定理,指的是在一个分布式系统中,**`Consistency`**(一致性)、**`Availability`** (可用性)、**`Partition tolerance`**(分区容错性),三者不可兼得。 

* **一致性(C)**：分布式系统中多个主机之间是否能够保持数据一致的特性。即,当系统数据发生更新操作后,各个主机中的数据仍然处于一致的状态。
* **可用性(A)**：系统提供的服务必须一直处于可用的状态,即对于用户的每一个请求,系统总是可以**在有限的时间内**对用户**做出响应**。
* **分区容错性(P)**：分布式系统在遇到任何网络分区故障时,仍能够保证对外提供满足一致性或可用性的服务。 

## 1.5.2 三二原则

​	对于分布式系统,在 **CAP** 原则中分区容错性 **P** 是必须要保证的。但其并不能<u>同时</u>保证**一致性**与**可用性**。因为现在的分布式系统在满足了一致性的前提下,会牺牲可用性。因为, 为了保证一致性,系统中各个节点需要进行分布式同步,而在分布式同步过程中,整个系统 是不对外提供服务的,即无法保证了可用性;在满足了可用性的前提下,会牺牲一致性。因 为,只要系统某一节点上的数据有更新,其它节点必然要做数据同步,在同步过程中系统中 各个节点中的数据是不一致的,而这个同步过程,整个系统仍是对外提供服务的。即牺牲了一致性保证了可用性。 

​	因此,**CAP 原则对于一个分布式系统来说,只可能满足两项,即要么 CP,要么 AP**。这就 是 CAP 的三二原则。 

## 1.5.3 BASE理论

​	**BASE** 是 `Basically Available`(基本可用)、`Soft state`(软状态)和 `Eventually consistent`(最终一致性)三个短语的简写,**BASE** 是对 **CAP** 中一致性和可用性权衡、折中的结果方案,其来源于对大规模互联网系统分布式实践的结论,是基于 CAP 定理逐步演化而来的,其核心思想是即使 无法做到强一致性,但每个应用都可以根据自身的业务特点,采用适当的方式来使系统达到最终一致性。 

### (1) 基本可用 

​	基本可用是指分布式系统在出现不可预知故障的时候,**允许损失部分可用性**。 

### (2) 软状态 

​	软状态,是指允许系统中的数据存在的中间状态,并认为该中间状态的存在不会影响系统的整体可用性,即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。软状态,其实就是**一种灰度状态,过渡状态**。 

### (3) 最终一致性 

​	最终一致性强调的是系统中所有的数据副本,在经过一段时间的同步后,最终能够达到一个一致的状态。因此,最终一致性的本质是**需要系统保证最终数据能够达到一致,而不需要实时保证系统数据的强一致性**。 

​	在没有发生故障的前提下,数据达到一致状态的时间延迟,取决于网络延迟,系统负载和数据复制方案设计等因素。 

## 1.5.4 ZK与CP

​	**zk 遵循的是 CP 原则,即保证了一致性,但牺牲了可用性**。体现在哪里呢? 

​	当 **Leader** 宕机后,**zk** 集群会马上进行新的 **Leader** 的选举。但选举时长在 `30-120` 秒间, 整个选举期间 **zk** 集群是不接受客户端的读写操作的,即 zk 集群是处于瘫痪状态的。所以, 其不满足可用性。 

​	为什么 **Leader** 的选举需要这么长的时间呢?为了保证 zk 集群各个节点中数据的一致性, zk 集群做了两类数据同步：**初始化同步**与**更新同步**。当新的 **Leader** 被选举出后,各个 **Follower** 需要将新 **Leader** 的数据同步到自己的缓存中,这是初始化同步;当 **Leader** 的数据被客户端修改后,其会向 **Follower** 发出广播,然后各个 **Follower** 会主动同步 **Leader** 的更新数据,这是更新同步。无论是初始化同步还是更新同步,zk 集群为了保证数据的一致性,若发现超 过半数的 **Follower** 同步超时,则其会再次进行同步,而这个过程中 zk 集群是处于不可用状态的。 

​	<u>由于 **zk** 采用了 **CP** 原则,所以导致其可用性降低,这是其致命的问题</u>。**Spring Cloud** 的**Eureka** 在分布式系统中所起的作用类似于 **zk**,但其采用了 **AP** 原则,其牺牲了一致性,但保证了可用性。 

# 1.6 重点概念回顾

1. 关于活锁的叙述正确的是（a）
   a、Paxos算法存在活锁问题；
   b、Fast Paxos算法存在活锁问题；
   c、ZAB算法存在活锁问题

   > Paxos算法纯粹就是个理论，实际应用中有不足之处，后续对其有所改进，比如Fast Paxos算法，解决了活锁问题；ZAB算法就是Fast Paxos算法的一种工业实现，ZK中的应用实现；
   >
   > 即：Fast Paxos是对Paxos的改进，解决活锁问题；ZAB协议是 `Fast Paxos` 算法在ZK中的一种具体应用实现。

2. 在提案投票中具有投票权的是（a）
   a、follower
   b、observer
   c、learner
   d、quorum

   > observer其实就是没有选举权和被选举权的follower（相当于单位里的临时工，只干活，没有员工福利）；
   >
   > learner其实就是follower+observer
   >
   > quorum是法定人，其实就是follower+leader（leader是自己投自己）

3. 对zk集群进行水平扩展时，若我们增加对是Follower，其对系统性能的影响是（c）
   a、同时降低了读／写操作效率；
   b、降低了读操作效率，提高了写操作效率；
   c、降低了写操作效率，提高了读操作效率；
   d、仅提高了写操作效率，对读操作影像不大；
   e、仅提高了读操纵效率，对写操作影像不大；

   > 增加了投票的时间成本，因此写的效率降低；

4. prepare与accept阶段的工作过程差不多，为什么需要prepare过程呢？paxos算法在设计初为什么不直接accept阶段呢？ 

   > 1. prepare阶段是一个**“征求意见”阶段**，询问所有的Acceptor是否同意该提案，同意者反馈给Proposer一个ACK。该阶段向所有Acceptor发送的是一个全局唯一编号N。
   > 2. accept阶段是**提案执行阶段**，只有当收到超过半数的ACK后才会让所有Learner同步提案。该阶段向所有Learner发送的是包含编号N的提案。
   > 3. 这两个阶段功能是不同的，发送的消息是不同的。若将两个阶段合并为一个阶段将无法进行“意见征求”，无法统计ACK。

5. 为什么在prepare阶段proposer发送的消息仅仅是一个编号N，而accept阶段发送的消息为proposal？反过来可以吗？ 

   > prepare阶段发送的消息是编号N，数据量很小；accept阶段发送的消息是proposal，数据量可能会较大。Paxos算法之所以先发送小数据量消息，是因为prepare阶段有可能是通不过的。若通过大数据量的消息进行“征求意见”，但最终没有通过，就会量费网络宽带等资源。